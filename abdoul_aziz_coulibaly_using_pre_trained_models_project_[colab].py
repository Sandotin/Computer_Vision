# -*- coding: utf-8 -*-
"""Abdoul Aziz Coulibaly - Using Pre-Trained Models Project [Colab]

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BTw1YowItA9W4Ibl5wFEtQV1NZh_uNe5

#### Copyright 2019 Google LLC.
"""

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""# Using Pre-Trained Models Project

In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object and then reassemble the video so that the boxes are shown around objects in the video.

## Overview

### Learning Objectives

* Use OpenCV to process images and video.
* Use a pre-trained model to identify and label objects in each frame of a video.
* Make judgements about classification quality and when to apply predicted labels.

### Prerequisites

* Classification
* Saving and Loading Models
* OpenCV
* Video Processing

### Estimated Duration

330 minutes (285 minutes working time, 45 minutes for presentations)

### Deliverables

1. A copy of this Colab notebook containing your code and responses to the ethical considerations below. The code should produce a functional labeled video.
1. A group presentation. After everyone is done, we will ask each group to stand in front of the class and give a brief presentation about what they have done in this lab. The presentation can be a code walkthrough, a group discussion, a slide show, or any other means that conveys what you did over the course of the day and what you learned. If you do create any artifacts for your presentation, please share them in the class folder.

### Grading Criteria

This project is graded in separate sections that each contribute a percentage of the total score:

1. Building and Using a Model (80%)
1. Ethical Implications (10%)
1. Project Presentation (10%)

#### Building and Using a Model

There are 6 demonstrations of competency listed in the problem statement below. Each competency is graded on a 3 point scale for a total of 18 available points. The following rubric will be used:

| Points | Description |
|--------|-------------|
| 0      | No attempt at the competency |
| 1      | Attempted competency, but in an incorrect manner |
| 2      | Attempted competency correctly, but sub-optimally |
| 3      | Successful demonstration of competency |


#### Ethical Implications

There are six questions in the **Ethical Implications** secion. Each question is worth 2 points. The rubric for calculating those points is:

| Points | Description |
|--------|-------------|
| 0      | No attempt at question or answer was off-topic or didn't make sense |
| 1      | Question was answered, but answer missed important considerations  |
| 2      | Answer adequately considered ethical implications |

#### Project Presentation

The project presentation will be graded on participation. All members of a team should actively participate.

## Team

Please enter your team members names in the placeholders in this text area:

*   *Abdoul Aziz Sandotin Coulibaly*
*   *Chris O'Rourke*
*   *Amanda Ma*

# Exercises

## Exercise 1: Coding

For this workshop you will process a video frame-by-frame, identify objects in each frame, and draw a bounding box and label around each object.
 
Use the [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) 'ssd_mobilenet_v1_coco' model. The video that you'll be processing can be [found on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). The 640x360 version of the video is smallest and easiest to handle, though any should work since you must scale down the images for processing.

The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.

### Student Solution
"""

# Dependencies
import cv2 as cv
import os
import PIL
import matplotlib as plt
import matplotlib.pyplot as plt
from PIL import ImageOps
from PIL import Image
import numpy as np
import urllib.request
import os
import tarfile
import shutil
import tensorflow as tf
from tqdm import tqdm_notebook as tqdm  #Progress bar
import scipy.misc

#Obtain the Model File
base_url = 'http://download.tensorflow.org/models/object_detection/'
file_name = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'
url = base_url + file_name
urllib.request.urlretrieve(url, file_name)
os.listdir()

#Extract the Model Data
dir_name = file_name[0:-len('.tar.gz')]
if os.path.exists(dir_name):
  shutil.rmtree(dir_name) 
tarfile.open(file_name, 'r:gz').extractall('./')
os.listdir(dir_name)

labels = {}
with open('labels.txt', 'r') as labelsFile:
  for line in labelsFile.readlines():
    split = line.strip().split(': ')
    labels[int(split[0])] = split[1]
  print(labels)

#Input Video
input_video = cv.VideoCapture('car.mp4')

height = int(input_video.get(cv.CAP_PROP_FRAME_HEIGHT))
width = int(input_video.get(cv.CAP_PROP_FRAME_WIDTH))
fps = 20 
# input_video.get(cv.CAP_PROP_FPS)
total_frames = int(input_video.get(cv.CAP_PROP_FRAME_COUNT))
print(total_frames)
#Defining the output video
fourcc = cv.VideoWriter_fourcc(*'mp4v')
output_video = cv.VideoWriter('cars-detection.mp4', fourcc, fps, (300, 300))

#First Frame
input_video.set(cv.CAP_PROP_POS_FRAMES, 0)
ret, frame = input_video.read()
#Shape of image
frame_width_height = frame.shape
# Find the longer dimension 
max_dimension = max(frame_width_height)

# Compute the delta width and height
width_padding = max_dimension - frame_width_height[1]
height_padding = max_dimension - frame_width_height[0]

# Compute the padding amounts
left_padding = width_padding // 2
right_padding = width_padding - left_padding
top_padding = height_padding // 2
bottom_padding = height_padding - top_padding

# Pad and plot the image
padding = (left_padding,top_padding,right_padding,bottom_padding)

#Reduce video to a short 25-fps video by grabbing the first frame of every second
for current_frame in tqdm(range(0, total_frames, int(fps))):
  input_video.set(cv.CAP_PROP_POS_FRAMES, current_frame)
  ret, frame_ = input_video.read()
  if not ret:
    raise Exception("Problem reading frame", i, " from video")

##################################################################

  #Convert the frame to an Image
  frame_ = Image.fromarray(frame_)

  #Apply padding to image
  padded_frame = ImageOps.expand(frame_, padding, (255,255,255,255))

  #Resize the image
  desired_size = (300, 300)
  resized_frame = padded_frame.resize(desired_size, Image.ANTIALIAS)
  
#######################################################################
  #Load the frozen graph
  frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')
  with tf.gfile.FastGFile(frozen_graph,'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

  #Convert the Image back to a numpy array
  image_np = np.asarray(resized_frame, dtype="int32")
  input_images = [image_np]

  #Shape of image
  width, height, _ = image_np.shape

  outputs = ('num_detections','detection_classes','detection_scores','detection_boxes',)

  #Run the graph
  with tf.Session() as sess:
    sess.graph.as_default()
    tf.import_graph_def(graph_def, name='')

    detections = sess.run([sess.graph.get_tensor_by_name(f'{op}:0') for op in outputs],
        feed_dict={ 'image_tensor:0': input_images })
  #Output
  num_detections = detections[0]
  detection_classes = detections[1]
  detection_scores = detections[2]
  detection_boxes = detections[3]

  #Number of object detected
  num_of_items = int(num_detections[0].item())

#######################################################################
  #Iterate through the number of object detected
  for i in range (num_of_items):

    #Label and Boundary features
    red = 255
    green = 255
    blue = 255
    scale = 0.5
    thickness = 2
    label = labels[detections[1][0][i]]

    #Object is a Car set color to blue
    if(detections[1][0][i] == 3):
      red = 0
      green = 0
      blue = 255

    #Object is a Truck set color to green
    if(detections[1][0][i] == 8):
      red = 0
      blue = 0
      green = 255

    #Dimensions of the Rectangle
    top = int(height*detections[3][0][i][0])
    left = int(width*detections[3][0][i][1])
    bottom = int(height*detections[3][0][i][2])
    right = int(width*detections[3][0][i][3])

    #Draw Rectangle
    cv.rectangle(image_np, (left, top), (right, bottom), (red, green, blue), thickness=1)
    
    #Draw Label
    cv.putText(image_np, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, scale, [red, green, blue], thickness)
    
    #making the video by outputting
  output_video.write(np.uint8(image_np))
input_video.release()
output_video.release()

"""## Exercise 2: Ethical Implications

Even the most basic of models have the potential to affect segments of the population in different ways. It is important to consider how your model might positively and negative effect different types of users.

In this section of the project you will reflect on the positive and negative implications of your model.

Frame the context of your models creation using this narriative:\n",

  > The city of Seattle is attempting to reduce traffic congestion in their downtown area. As part of this project, they plan on allowing each local driver one free downtown trip per week. After that the driver will have to pay a $50 toll for each extra day per week driven. As an early proof-of-concept for this project your team is tasked with using machine learning to correctly identify automobiles on the road. The next phase of the project will involve detecting license plate numbers and then cross-referencing that data with RFID chips that should be mounted in all local drivers cars."

### Student Solution

**Positive Impact**

Your model is trying to solve a problem. Think about who will benefit from that problem being solved and write a brief narrative about how the model will help.

---

Society will benefit from this technology as it will reduce traffic congestion. Because the model helps identify individual cars, who break the law, the city itself could make some money out of the fines. Imagine 100 people break the law every day, in a week,  the city will make 35 000 dollars. In a year, we have $1 820 000, which could be used to actually build more infrastructures and roads. 

Automated tracking is fast at detecting and classifying fast-moving objects on the road; faster than the human eye. 

Additionally, through implementing ml-backed vehicle tracking devices, we could divert funds from video security persons towards other meaningful projects.

**Negative Impact**

Models don't often have universal benefit. Think about who might be negatively impacted by the predictions your model is making. This person or persons might not be directly using the model, but instead might be impacted indirectly.

---

Machine learning models are not always accurate to begin with. A vehicle could easily be misclassfied as another vehicle, resulting in faulty fines. Additionally, if a vehicle is not detected, the city could lose money.

**Bias**

Models can be bias for many reasons. The bias can come from the data used to build the model (eg. sampling, data collection methods, available sources) and from the interpretation of the predictions generated by the model.

Think of at least two ways that bias might have been introduced to your model and explain both below.

---

The ML model could not be completely accurate due to abnormal/extreme weather conditions such as storms, tornadoes, cyclones, volcano eruptions, tsunamis, or nuclear bombing. Extreme weather makes cars and license plates harder to detect. 

Our model could be biased towards varying colored cars and can  misscharacterize them, like in our model it labeled a black car as a cellphone.

**Changing the Dataset to Mitigate Bias**

Bias datasets are one of the primary ways in which bias is introduced to a machine learning model. Look back at the input data that you fed to your model. Think about how you might change something about the data to reduce bias in your model.

What change or changes could you make to your dataset less bias? Consider the data that you have, how and where that data was collected, and what other sources of data might be used to reduce bias.

Write a summary of change that could be made to your input data.

---

Since the dataset is in video format consisting of frames, which are dependent upon a camera's resolution, if we increase the resolution, we can increase the accuracy in characterizing vehicles and license plates. 

We could increase the size of the dataset by capturing vehicles at multiple angles. This will increase variation in the data and possibly increase detection accuracy.

**Changing the Model to Mitigate Bias**

Is there any way to reduce bias by changing the model itself? This could include modifying algorithmic choices, tweaking hyperparameters, etc.

Write a brief summary of changes that you could make to help reduce bias in your model.

---

The model could add a feature to detect the person driving the car, so that the model can cross-reference the car and the driver detected to make a more accurate prediction reducing bias.

**Mitigating Bias Downstream**

Models make predictions. Downstream processes make decisions. What processes and/or rules should be in place for people and systems interpreting and acting on the results of your model to reduce the bias? Describe these below.

---

Since models can misclassify vehicles and result in faulty fines, people should be given the right to dispute fines or file claims. 

Addionally, we can make the model's prediction probablity threshold increased to 90% or more so that it can be more certain that the prediction is correct.
"""